{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6VqeAXYYrxhFk5VirVbDt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ighoshsubho/NLP_Question_Generation/blob/main/NLP_True_False_Question_Generation_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q4LYMR4gexpX"
      },
      "outputs": [],
      "source": [
        "str = 'React is the best frontend language. React libraries are very user friendly. React renders the pages statically.'\n",
        "ls = [x.strip() for x in str.strip()[:-1].split('.')]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install allennlp==0.9.0\n",
        "!pip install overrides==4.1.2"
      ],
      "metadata": {
        "id": "eLNH3zURike3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "PHoJbJDXipoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Split a sentence at ending noun phrase or verb phrase\n"
      ],
      "metadata": {
        "id": "tPCLb_PwlQPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "PdO24YyAjaVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz\")"
      ],
      "metadata": {
        "id": "Wzc03Um0jhrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = ls[0]\n",
        "test_sentence = test_sentence.rstrip('?:!.,;')\n",
        "print (test_sentence)\n",
        "parser_output = predictor.predict(sentence=test_sentence)\n",
        "print (parser_output)"
      ],
      "metadata": {
        "id": "dGHg6rpajkr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree_string = parser_output[\"trees\"]\n",
        "print (tree_string)"
      ],
      "metadata": {
        "id": "9IQhmT-Hjxqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import tokenize\n",
        "from nltk.tree import Tree\n",
        "\n",
        "tree = Tree.fromstring(tree_string)\n",
        "print (tree)\n",
        "print (tree.pretty_print())"
      ],
      "metadata": {
        "id": "GNnmYnEKjzuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Notations\n",
        "\n",
        "S\tsentence\t\n",
        "\n",
        "NP\tnoun phrase\t\n",
        "\n",
        "VP\tverb phrase\t\n",
        "\n",
        "PP\tprepositional phrase\t\n",
        "\n",
        "Det\tdeterminer\t\n",
        "\n",
        "N\tnoun\t\n",
        "\n",
        "V\tverb\t\n",
        "\n",
        "P\tpreposition\t\n",
        "\n",
        "VBD - Past Tense Verb\n",
        "\n",
        "JJ - Adjective\n",
        "\n",
        "etc"
      ],
      "metadata": {
        "id": "u_HFNjdNj9kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree.pretty_print()\n",
        "temp1 = tree[0]\n",
        "temp2 = tree[1]\n",
        "temp3 = tree[-1]\n",
        "temp1.pretty_print()\n",
        "temp2.pretty_print()\n",
        "temp3.pretty_print()"
      ],
      "metadata": {
        "id": "Bd4IA5jRj170"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split at right most nounphrase or verbphrase\n",
        "\n",
        "def get_flattened(t):\n",
        "    sent_str_final = None\n",
        "    if t is not None:\n",
        "        sent_str = [\" \".join(x.leaves()) for x in list(t)]\n",
        "        sent_str_final = [\" \".join(sent_str)]\n",
        "        sent_str_final = sent_str_final[0]\n",
        "    return sent_str_final\n",
        "\n",
        "def get_right_most_VP_or_NP(parse_tree,last_NP = None,last_VP = None):\n",
        "    if len(parse_tree.leaves()) == 1:\n",
        "        return last_NP,last_VP\n",
        "    last_subtree = parse_tree[-1]\n",
        "    if last_subtree.label() == \"NP\":\n",
        "        last_NP = last_subtree\n",
        "    elif last_subtree.label() == \"VP\":\n",
        "        last_VP = last_subtree\n",
        "    \n",
        "    return get_right_most_VP_or_NP(last_subtree,last_NP,last_VP)\n",
        "\n",
        "\n",
        "last_nounphrase, last_verbphrase =  get_right_most_VP_or_NP(tree)\n",
        "last_nounphrase_flattened = get_flattened(last_nounphrase)\n",
        "last_verbphrase_flattened = get_flattened(last_verbphrase)\n",
        "\n",
        "print (\"Original Sentence \",test_sentence)\n",
        "print (\"last_nounphrase \",last_nounphrase )\n",
        "print (\"last_verbphrase \",last_verbphrase)\n",
        "print (\"\\n \")\n",
        "print (\"last_nounphrase \",last_nounphrase_flattened )\n",
        "print (\"last_verbphrase \",last_verbphrase_flattened)"
      ],
      "metadata": {
        "id": "nUnFU0GIkG2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# sub_string - sipping coffee\n",
        "# main_string - The old woman was sitting under a tree and sipping coffee\n",
        "# compare like below\n",
        "# Theoldwomanwassittingunderatreeandsippingcoffee  || sippingcoffee\n",
        "# oldwomanwassittingunderatreeandsippingcoffee || sippingcoffee\n",
        "# womanwassittingunderatreeandsippingcoffee || sippingcoffee\n",
        "# ...............\n",
        "# andsippingcoffee || sippingcoffee\n",
        "# sippingcoffee || sippingcoffee\n",
        "def get_termination_portion(main_string, sub_string):\n",
        "    combined_sub_string = sub_string.replace(\" \", \"\")\n",
        "    main_string_list = main_string.split()\n",
        "    last_index = len(main_string_list)\n",
        "    for i in range(last_index):\n",
        "        check_string_list = main_string_list[i:]\n",
        "        check_string = \"\".join(check_string_list)\n",
        "        check_string = check_string.replace(\" \", \"\")\n",
        "        if check_string == combined_sub_string:\n",
        "            return \" \".join(main_string_list[:i])\n",
        "\n",
        "    return None\n",
        "\n",
        "longest_phrase_to_use = max(last_nounphrase_flattened, last_verbphrase_flattened,key = len)\n",
        "print (\"Ending phrase: \", longest_phrase_to_use)\n",
        "\n",
        "longest_phrase_to_use = re.sub(r\"-LRB- \", \"(\", longest_phrase_to_use)\n",
        "longest_phrase_to_use = re.sub(r\" -RRB-\", \")\", longest_phrase_to_use)\n",
        "\n",
        "\n",
        "split_sentence = get_termination_portion(test_sentence, longest_phrase_to_use)\n",
        "print (\"Original sentence : \",test_sentence)\n",
        "print (\"Original sentence after splitting at ending phrase: \",split_sentence)"
      ],
      "metadata": {
        "id": "UfEKSNGgkKU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split at the first noun phrase or verb phrase\n",
        "\n",
        "test_sentence2 = ls[1]\n",
        "test_sentence2 = test_sentence2.rstrip('?:!.,;')\n",
        "print (test_sentence2)\n",
        "parser_output2 = predictor.predict(sentence=test_sentence2)\n",
        "tree_string2 = parser_output2[\"trees\"]\n",
        "\n",
        "tree2 = Tree.fromstring(tree_string2)\n",
        "print (tree2.pretty_print())"
      ],
      "metadata": {
        "id": "8mNBkaxgkP_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SBAR stands for Subordinate Clause.\n",
        "#  Penn Tree bank overview - http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.8216&rep=rep1&type=pdf\n",
        "\n",
        "def get_first_VP_and_NP_and_sentence(parse_tree, first_NP=None, first_VP=None, first_sent=None):\n",
        "    if len(parse_tree.leaves()) == 1:\n",
        "        return get_flattened(first_NP), get_flattened(first_VP), get_flattened(first_sent)\n",
        "    last_subtree = parse_tree[-1]\n",
        "\n",
        "    if last_subtree.label() == \"NP\" and not first_NP:\n",
        "        first_NP = last_subtree\n",
        "    elif last_subtree.label() == \"VP\" and not first_VP:\n",
        "        first_VP = last_subtree\n",
        "    elif last_subtree.label() == \"S\" and not first_sent:\n",
        "        first_sent = last_subtree\n",
        "\n",
        "    return get_first_VP_and_NP_and_sentence(last_subtree, first_NP, first_VP, first_sent)\n",
        "\n",
        "\n",
        "first_nounphrase, first_verbphrase, first_sentence = get_first_VP_and_NP_and_sentence(tree2)\n",
        "\n",
        "print(\"first_nounphrase: \",first_nounphrase)\n",
        "print (\"first_verbphrase: \",first_verbphrase)\n",
        "print (\"first_sentence: \",first_sentence)"
      ],
      "metadata": {
        "id": "QO8GyNk-kx51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "longest_phrase_to_use = max(first_nounphrase, first_verbphrase,key = len)\n",
        "print (\"Ending phrase: \", longest_phrase_to_use)\n",
        "\n",
        "longest_phrase_to_use = re.sub(r\"-LRB- \", \"(\", longest_phrase_to_use)\n",
        "longest_phrase_to_use = re.sub(r\" -RRB-\", \")\", longest_phrase_to_use)\n",
        "\n",
        "\n",
        "split_sentence = get_termination_portion(test_sentence2, longest_phrase_to_use)\n",
        "print (\"Original sentence : \",test_sentence2)\n",
        "print (\"Original sentence after splitting at ending phrase: \",split_sentence)"
      ],
      "metadata": {
        "id": "pHMYSGHLk_8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Generate alternate endings to a split sentence using GPT-2"
      ],
      "metadata": {
        "id": "dsYyeuWMlZ-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "vkHQIelFlGrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# GPT2tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
        "# GPT2model = TFGPT2LMHeadModel.from_pretrained(\"distilgpt2\",pad_token_id=GPT2tokenizer.eos_token_id)\n",
        "GPT2tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "GPT2model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\",pad_token_id=GPT2tokenizer.eos_token_id)"
      ],
      "metadata": {
        "id": "GHChmNoAldxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "partial_sentence = ls[0]\n",
        "input_ids = GPT2tokenizer.encode(partial_sentence,return_tensors='tf')\n",
        "print (input_ids)\n",
        "maximum_length = len(partial_sentence.split())+40"
      ],
      "metadata": {
        "id": "oinsMuCylhIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activate top_k sampling and top_p sampling with only from 90% most likely words\n",
        "sample_outputs = GPT2model.generate(\n",
        "    input_ids, \n",
        "    do_sample=True, \n",
        "    max_length=maximum_length, \n",
        "    top_p=0.80, # 0.85 \n",
        "    top_k=30,   #30\n",
        "    repetition_penalty  = 10.0,\n",
        "    num_return_sequences=5\n",
        ")"
      ],
      "metadata": {
        "id": "-_ZavlOQlmGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import tokenize\n",
        "generated_sentences=[]\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "    decoded_sentence = GPT2tokenizer.decode(sample_output, skip_special_tokens=True)\n",
        "    # final_sentence = decoded_sentence\n",
        "    final_sentence = tokenize.sent_tokenize(decoded_sentence)[0]\n",
        "    generated_sentences.append(final_sentence)\n",
        "    print (i,\": \",final_sentence)"
      ],
      "metadata": {
        "id": "BqpyVc6Blwst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Filter sentences with BERT"
      ],
      "metadata": {
        "id": "4aHhu5knl7cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pypi.org/project/sentence-transformers/\n",
        "!pip install sentence-transformers==0.4.0"
      ],
      "metadata": {
        "id": "wy4VBCrBl4Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "BERT_model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')"
      ],
      "metadata": {
        "id": "7YN9DlCQmEQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "possible_false_sentences = generated_sentences\n",
        "\n",
        "\n",
        "original_sentence = ls[0]\n",
        "\n",
        "print(original_sentence)"
      ],
      "metadata": {
        "id": "MAO1ZT_tmGWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "false_sentences_embeddings = BERT_model.encode(possible_false_sentences)\n",
        "original_sentence_embedding = BERT_model.encode([original_sentence])"
      ],
      "metadata": {
        "id": "k5PtD88LmfYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy\n",
        "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html\n",
        "distances = scipy.spatial.distance.cdist(original_sentence_embedding, false_sentences_embeddings, \"cosine\")[0]\n",
        "print (distances)"
      ],
      "metadata": {
        "id": "tJyv1_eNmgFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = zip(range(len(distances)), distances)\n",
        "results = sorted(results, key=lambda x: x[1])\n",
        "print (results)"
      ],
      "metadata": {
        "id": "LP2jZ7pkmjR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dissimilar_sentences =[]\n",
        "for idx, distance in results:\n",
        "  dissimilar_sentences.append(possible_false_sentences[idx])\n",
        "  print (possible_false_sentences[idx])"
      ],
      "metadata": {
        "id": "Jh4QNAESmnpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "false_sentences_list_final = reversed(dissimilar_sentences)\n",
        "for sent in false_sentences_list_final:\n",
        "  print (sent)"
      ],
      "metadata": {
        "id": "-LWlDdivmqRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Saving the falsified statement and true ones in a list randomly and feeding it in t5 model for proper question generation"
      ],
      "metadata": {
        "id": "zwClnK9tmtxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/mrm8488/t5-base-finetuned-question-generation-ap\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\t\n",
        "output = query({\n",
        "\t\"inputs\": f\"answer: True context: {ls[2]}.\",\n",
        "})\n",
        "\n",
        "print(output[0]['generated_text'][10:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9IUC5ejgpbm",
        "outputId": "8493c0f7-afc4-458b-bb13-21dff0bd188c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the default state of the page rendering?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_true_output = []\n",
        "for x in ls:\n",
        "  def query(payload):\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    return response.json()\n",
        "\t\n",
        "  output = query({\n",
        "    \"inputs\": f\"answer: True context: {x}.\",\n",
        "  })\n",
        "\n",
        "  final_true_output.append(output[0]['generated_text'][10:])\n",
        "\n",
        "print(final_true_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQ9BlfaUydYv",
        "outputId": "bf28b51a-81a2-4f12-caf6-e78219d6cd66"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Is React the best frontend language?', 'Are React libraries user friendly?', 'What is the default state of the page rendering?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Performing all the steps sequentially for generting falsified statements:"
      ],
      "metadata": {
        "id": "WY6k6x4h2TtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "import locale\n",
        "print(locale.getpreferredencoding())\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FgvGNnY8JwR",
        "outputId": "34df6cc9-0e9d-44c8-9e8f-eefa736b75c6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UTF-8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install allennlp"
      ],
      "metadata": {
        "id": "ZzLIn68C8Jmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from allennlp.predictors.predictor import Predictor\n",
        "from nltk import tokenize\n",
        "from nltk.tree import Tree\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import tokenize\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import scipy\n",
        "\n",
        "BERT_model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
        "\n",
        "def get_flattened(t):\n",
        "    sent_str_final = None\n",
        "    if t is not None:\n",
        "        sent_str = [\" \".join(x.leaves()) for x in list(t)]\n",
        "        sent_str_final = [\" \".join(sent_str)]\n",
        "        sent_str_final = sent_str_final[0]\n",
        "    return sent_str_final\n",
        "\n",
        "def get_right_most_VP_or_NP(parse_tree,last_NP = None,last_VP = None):\n",
        "    if len(parse_tree.leaves()) == 1:\n",
        "        return last_NP,last_VP\n",
        "    last_subtree = parse_tree[-1]\n",
        "    if last_subtree.label() == \"NP\":\n",
        "        last_NP = last_subtree\n",
        "    elif last_subtree.label() == \"VP\":\n",
        "        last_VP = last_subtree\n",
        "    \n",
        "    return get_right_most_VP_or_NP(last_subtree,last_NP,last_VP)\n",
        "\n",
        "def get_termination_portion(main_string, sub_string):\n",
        "    combined_sub_string = sub_string.replace(\" \", \"\")\n",
        "    main_string_list = main_string.split()\n",
        "    last_index = len(main_string_list)\n",
        "    for i in range(last_index):\n",
        "        check_string_list = main_string_list[i:]\n",
        "        check_string = \"\".join(check_string_list)\n",
        "        check_string = check_string.replace(\" \", \"\")\n",
        "        if check_string == combined_sub_string:\n",
        "            return \" \".join(main_string_list[:i])\n",
        "\n",
        "    return None\n",
        "\n",
        "def get_first_VP_and_NP_and_sentence(parse_tree, first_NP=None, first_VP=None, first_sent=None):\n",
        "    if len(parse_tree.leaves()) == 1:\n",
        "        return get_flattened(first_NP), get_flattened(first_VP), get_flattened(first_sent)\n",
        "    last_subtree = parse_tree[-1]\n",
        "\n",
        "    if last_subtree.label() == \"NP\" and not first_NP:\n",
        "        first_NP = last_subtree\n",
        "    elif last_subtree.label() == \"VP\" and not first_VP:\n",
        "        first_VP = last_subtree\n",
        "    elif last_subtree.label() == \"S\" and not first_sent:\n",
        "        first_sent = last_subtree\n",
        "\n",
        "    return get_first_VP_and_NP_and_sentence(last_subtree, first_NP, first_VP, first_sent)\n",
        "\n",
        "# GPT2tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "# GPT2model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\",pad_token_id=GPT2tokenizer.eos_token_id)\n",
        "\n",
        "GPT2tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "GPT2model = GPT2Model.from_pretrained('gpt2-medium')\n",
        "\n",
        "generated_sentences=[]\n",
        "final_falsified_output = []"
      ],
      "metadata": {
        "id": "7EGPP4O68JdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install allennlp-models"
      ],
      "metadata": {
        "id": "C--Wfw0veMec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2020.02.27.tar.gz\")"
      ],
      "metadata": {
        "id": "-JTFd9uZK_Fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install allennlp-models==2.0.1.dev20210201"
      ],
      "metadata": {
        "id": "iqgnoKfUntEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from allennlp_models.pretrained import load_predictor\n",
        "for x in ls:\n",
        "\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "  test_sentence = x\n",
        "  test_sentence = test_sentence.rstrip('?:!.,;')\n",
        "\n",
        "  parser_output = predictor.predict(premise=test_sentence)\n",
        "\n",
        "  tree_string = parser_output[\"trees\"]\n",
        "\n",
        "  tree = Tree.fromstring(tree_string)\n",
        "\n",
        "  last_nounphrase, last_verbphrase =  get_right_most_VP_or_NP(tree)\n",
        "  last_nounphrase_flattened = get_flattened(last_nounphrase)\n",
        "  last_verbphrase_flattened = get_flattened(last_verbphrase)\n",
        "\n",
        "  longest_phrase_to_use = max(last_nounphrase_flattened, last_verbphrase_flattened,key = len)\n",
        "  print (\"Ending phrase: \", longest_phrase_to_use)\n",
        "\n",
        "  longest_phrase_to_use = re.sub(r\"-LRB- \", \"(\", longest_phrase_to_use)\n",
        "  longest_phrase_to_use = re.sub(r\" -RRB-\", \")\", longest_phrase_to_use)\n",
        "\n",
        "  # test_sentence2 = \"They had no ice cream left at home, nor did they have money to go to the store.\"\n",
        "  # test_sentence2 = test_sentence2.rstrip('?:!.,;')\n",
        "  # parser_output2 = predictor.predict(sentence=test_sentence2)\n",
        "  # tree_string2 = parser_output2[\"trees\"]\n",
        "\n",
        "  # tree2 = Tree.fromstring(tree_string2)\n",
        "\n",
        "  first_nounphrase, first_verbphrase, first_sentence = get_first_VP_and_NP_and_sentence(tree)\n",
        "\n",
        "  longest_phrase_to_use = max(first_nounphrase, first_verbphrase,key = len)\n",
        "  longest_phrase_to_use = re.sub(r\"-LRB- \", \"(\", longest_phrase_to_use)\n",
        "  longest_phrase_to_use = re.sub(r\" -RRB-\", \")\", longest_phrase_to_use)\n",
        "  split_sentence = get_termination_portion(test_sentence, longest_phrase_to_use)\n",
        "\n",
        "  partial_sentence = split_sentence\n",
        "  input_ids = GPT2tokenizer.encode(partial_sentence,return_tensors='tf')\n",
        "  maximum_length = len(partial_sentence.split())+40\n",
        "\n",
        "  sample_outputs = GPT2model.generate(\n",
        "      input_ids, \n",
        "      do_sample=True, \n",
        "      max_length=maximum_length, \n",
        "      top_p=0.80, # 0.85 \n",
        "      top_k=30,   #30\n",
        "      repetition_penalty  = 10.0,\n",
        "      num_return_sequences=5\n",
        "  )\n",
        "\n",
        "  for i, sample_output in enumerate(sample_outputs):\n",
        "      decoded_sentence = GPT2tokenizer.decode(sample_output, skip_special_tokens=True)\n",
        "      # final_sentence = decoded_sentence\n",
        "      final_sentence = tokenize.sent_tokenize(decoded_sentence)[0]\n",
        "      generated_sentences.append(final_sentence)\n",
        "\n",
        "  possible_false_sentences = generated_sentences\n",
        "\n",
        "  original_sentence = x\n",
        "\n",
        "  false_sentences_embeddings = BERT_model.encode(possible_false_sentences)\n",
        "  original_sentence_embedding = BERT_model.encode([original_sentence])\n",
        "\n",
        "  distances = scipy.spatial.distance.cdist(original_sentence_embedding, false_sentences_embeddings, \"cosine\")[0]\n",
        "\n",
        "  results = zip(range(len(distances)), distances)\n",
        "  results = sorted(results, key=lambda x: x[1])\n",
        "\n",
        "  dissimilar_sentences =[]\n",
        "  for idx, distance in results:\n",
        "    dissimilar_sentences.append(possible_false_sentences[idx])\n",
        "\n",
        "  false_sentences_list_final = reversed(dissimilar_sentences)\n",
        "\n",
        "  final_falsified_output.append(false_sentences_list_final[0])"
      ],
      "metadata": {
        "id": "XyPL-Uss8JU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_falsified_output)"
      ],
      "metadata": {
        "id": "8fx0sjOU0RnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Storing all the true and false statements in a list and jumbling up after zipping them with their values:"
      ],
      "metadata": {
        "id": "TGYcfu8d0YeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "Total_output = [] #final_falsified_output + final_true_output\n",
        "for x in final_falsified_output:\n",
        "  Total_output.append(zip(\"False\",x))\n",
        "for x in final_true_output:\n",
        "  Total_output.append(zip(\"True\",x))\n",
        "random.shuffle(Total_output)\n",
        "print(Total_output)"
      ],
      "metadata": {
        "id": "1e_98Ixc0tiZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}